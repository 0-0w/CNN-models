{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5dc55ac5-0566-4421-81f1-0a5cfd821dae",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Incorrect model/corpus name",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequence\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pad_sequences\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Load pre-trained Word2Vec model\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m word2vec \u001b[38;5;241m=\u001b[39m \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mword2vec-google-news-300.bin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m embedding_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m\n\u001b[0;32m     13\u001b[0m hidden_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\gensim\\downloader.py:492\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, return_path)\u001b[0m\n\u001b[0;32m    490\u001b[0m file_name \u001b[38;5;241m=\u001b[39m _get_filename(name)\n\u001b[0;32m    491\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 492\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncorrect model/corpus name\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    493\u001b[0m folder_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(BASE_DIR, name)\n\u001b[0;32m    494\u001b[0m path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder_dir, file_name)\n",
      "\u001b[1;31mValueError\u001b[0m: Incorrect model/corpus name"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, RepeatVector, TimeDistributed, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Load pre-trained Word2Vec model\n",
    "word2vec = api.load(\"word2vec-google-news-300.bin\")\n",
    "embedding_dim = 300\n",
    "hidden_dim = 64\n",
    "max_sentence_len = 20  # Max length for padding sequences\n",
    "\n",
    "# Assuming `sentences` is a list of strings with your dataset sentences\n",
    "# Replace with actual data loading\n",
    "sentences = [\"This is a sentence\", \"Another example sentence\", \"This is the third sentence\"]\n",
    "\n",
    "# Tokenize and embed sentences\n",
    "def embed_sentence(sentence):\n",
    "    tokens = word_tokenize(sentence.lower())\n",
    "    embeddings = [word2vec[word] for word in tokens if word in word2vec]\n",
    "    return embeddings\n",
    "\n",
    "embedded_sentences = [embed_sentence(sent) for sent in sentences]\n",
    "embedded_sentences = pad_sequences(embedded_sentences, maxlen=max_sentence_len, dtype='float32', padding='post')\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test = train_test_split(embedded_sentences, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model definition using Keras\n",
    "input_dim = (max_sentence_len, embedding_dim)\n",
    "\n",
    "input_layer = tf.keras.Input(shape=input_dim)\n",
    "# Encoder\n",
    "encoded = LSTM(hidden_dim, return_sequences=False)(input_layer)\n",
    "encoded = RepeatVector(max_sentence_len)(encoded)\n",
    "\n",
    "# Decoder\n",
    "decoded = LSTM(hidden_dim, return_sequences=True)(encoded)\n",
    "output_layer = TimeDistributed(Dense(embedding_dim))(decoded)\n",
    "\n",
    "# Compile model\n",
    "autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Model summary\n",
    "autoencoder.summary()\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 20\n",
    "batch_size = 32\n",
    "\n",
    "history = autoencoder.fit(X_train, X_train,\n",
    "                          epochs=num_epochs,\n",
    "                          batch_size=batch_size,\n",
    "                          validation_data=(X_test, X_test))\n",
    "\n",
    "# After training, you can inspect the modelâ€™s ability to reconstruct sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab32acf1-6b2e-4f17-aebf-ef9a37c048cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gensim.downloader as api\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, RepeatVector, TimeDistributed, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e99139be-5afb-46c5-b27a-ad40ac90b810",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 3.35 GiB for an array with shape (3000000, 300) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load the Word2Vec model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m word2vec \u001b[38;5;241m=\u001b[39m \u001b[43mKeyedVectors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_word2vec_format\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGoogleNews-vectors-negative300.bin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m embedding_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m\n\u001b[0;32m      4\u001b[0m hidden_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\gensim\\models\\keyedvectors.py:1719\u001b[0m, in \u001b[0;36mKeyedVectors.load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[0;32m   1672\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m   1673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_word2vec_format\u001b[39m(\n\u001b[0;32m   1674\u001b[0m         \u001b[38;5;28mcls\u001b[39m, fname, fvocab\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m'\u001b[39m, unicode_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1675\u001b[0m         limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, datatype\u001b[38;5;241m=\u001b[39mREAL, no_header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1676\u001b[0m     ):\n\u001b[0;32m   1677\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load KeyedVectors from a file produced by the original C word2vec-tool format.\u001b[39;00m\n\u001b[0;32m   1678\u001b[0m \n\u001b[0;32m   1679\u001b[0m \u001b[38;5;124;03m    Warnings\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1717\u001b[0m \n\u001b[0;32m   1718\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1719\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_word2vec_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1720\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbinary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43municode_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43municode_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1721\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatatype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatatype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_header\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mno_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1722\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\gensim\\models\\keyedvectors.py:2062\u001b[0m, in \u001b[0;36m_load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[0;32m   2060\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m limit:\n\u001b[0;32m   2061\u001b[0m     vocab_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(vocab_size, limit)\n\u001b[1;32m-> 2062\u001b[0m kv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvector_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatatype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2064\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[0;32m   2065\u001b[0m     _word2vec_read_binary(\n\u001b[0;32m   2066\u001b[0m         fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, binary_chunk_size, encoding\n\u001b[0;32m   2067\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\gensim\\models\\keyedvectors.py:245\u001b[0m, in \u001b[0;36mKeyedVectors.__init__\u001b[1;34m(self, vector_size, count, dtype, mapfile_path)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# pointer to where next new entry will land\u001b[39;00m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_to_index \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 245\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectors \u001b[38;5;241m=\u001b[39m \u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvector_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# formerly known as syn0\u001b[39;00m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorms \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# \"expandos\" are extra attributes stored for each key: {attribute_name} => numpy array of values of\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# this attribute, with one array value for each vector key.\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# The same information used to be stored in a structure called Vocab in Gensim <4.0.0, but\u001b[39;00m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;66;03m# with different indexing: {vector key} => Vocab object containing all attributes for the given vector key.\u001b[39;00m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;66;03m# Don't modify expandos directly; call set_vecattr()/get_vecattr() instead.\u001b[39;00m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 3.35 GiB for an array with shape (3000000, 300) and data type float32"
     ]
    }
   ],
   "source": [
    "# Load the Word2Vec model\n",
    "word2vec = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "embedding_dim = 300\n",
    "hidden_dim = 64\n",
    "max_sentence_len = 20  # Max sequence length for padding\n",
    "\n",
    "# Load MSRP Dataset\n",
    "# Replace 'msrp_train.txt' with the actual path to your MSRP dataset\n",
    "msrp_path = \"msr_paraphrase_data.csv\"\n",
    "msrp_data = pd.read_csv('msr_paraphrase_train.csv', on_bad_lines='skip')\n",
    "\n",
    "# Extract sentences\n",
    "# sentences = list(msrp_data['sentence1']) + list(msrp_data['sentence2'])\n",
    "sentences1 = data['#1 String'].fillna(\"\").astype(str).values \n",
    "sentences2 = data['#2 String'].fillna(\"\").astype(str).values\n",
    "sentences = list(sentences1) + list(sentences2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e2d357-dec7-4a8c-9852-9a64e8108419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess: Tokenize and embed sentences\n",
    "def embed_sentence(sentence):\n",
    "    tokens = word_tokenize(sentence.lower())\n",
    "    embeddings = [word2vec[word] for word in tokens if word in word2vec]\n",
    "    return embeddings\n",
    "\n",
    "# Convert sentences to embeddings\n",
    "embedded_sentences = [embed_sentence(sent) for sent in sentences]\n",
    "# Pad the sentences to have uniform length\n",
    "embedded_sentences = pad_sequences(\n",
    "    [np.array(e) for e in embedded_sentences if len(e) > 0], \n",
    "    maxlen=max_sentence_len, \n",
    "    dtype='float32', \n",
    "    padding='post'\n",
    ")\n",
    "\n",
    "# Split into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test = train_test_split(embedded_sentences, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the Autoencoder Model\n",
    "input_dim = (max_sentence_len, embedding_dim)\n",
    "\n",
    "# Define the model\n",
    "input_layer = tf.keras.Input(shape=input_dim)\n",
    "encoded = LSTM(hidden_dim, return_sequences=False)(input_layer)\n",
    "encoded = RepeatVector(max_sentence_len)(encoded)\n",
    "decoded = LSTM(hidden_dim, return_sequences=True)(encoded)\n",
    "output_layer = TimeDistributed(Dense(embedding_dim))(decoded)\n",
    "\n",
    "autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the Autoencoder\n",
    "num_epochs = 20\n",
    "batch_size = 32\n",
    "\n",
    "history = autoencoder.fit(X_train, X_train,\n",
    "                          epochs=num_epochs,\n",
    "                          batch_size=batch_size,\n",
    "                          validation_data=(X_test, X_test))\n",
    "\n",
    "# Evaluate\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a07679d-98ac-42e5-9bff-b7d6f752eaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss = history.history['loss']\n",
    "validation_loss = history.history['val_loss']\n",
    "training_accuracy = history.history['cosine_similarity']\n",
    "validation_accuracy = history.history['val_cosine_similarity']\n",
    "\n",
    "# Print final results\n",
    "print(f\"Final Training Loss: {training_loss[-1]}\")\n",
    "print(f\"Final Validation Loss: {validation_loss[-1]}\")\n",
    "print(f\"Final Training Accuracy (Cosine Similarity): {training_accuracy[-1]}\")\n",
    "print(f\"Final Validation Accuracy (Cosine Similarity): {validation_accuracy[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f361c776-b2ff-47fe-bfa3-292dc8376b2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
