{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab32acf1-6b2e-4f17-aebf-ef9a37c048cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, RepeatVector, TimeDistributed, Dense, Bidirectional,Input, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.metrics import CosineSimilarity\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from gensim.models import KeyedVectors\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from gensim.models import Word2Vec\n",
    "# import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aaeefb4b-4ee4-4735-9020-d1a92ce7d961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MSRP Dataset\n",
    "# Replace 'msrp_train.txt' with the actual path to your MSRP dataset\n",
    "# msrp_path = \"msr_paraphrase_data.csv\"\n",
    "data = pd.read_csv('msr_paraphrase_train.csv', on_bad_lines='skip')\n",
    "\n",
    "# Extract sentences\n",
    "# sentences = list(msrp_data['sentence1']) + list(msrp_data['sentence2'])\n",
    "sentences1 = data['#1 String'].fillna(\"\").astype(str).values \n",
    "sentences2 = data['#2 String'].fillna(\"\").astype(str).values\n",
    "sentences = list(sentences1) + list(sentences2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14961ea6-b674-4991-b46f-6940cabcdb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "word2vec = Word2Vec(sentences=tokenized_sentences, vector_size=300, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8e2d357-dec7-4a8c-9852-9a64e8108419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 36ms/step - cosine_similarity: 0.5612 - loss: 2.1451 - val_cosine_similarity: 0.9014 - val_loss: 0.0476\n",
      "Epoch 2/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - cosine_similarity: 0.8981 - loss: 0.0452 - val_cosine_similarity: 0.9020 - val_loss: 0.0441\n",
      "Epoch 3/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - cosine_similarity: 0.9017 - loss: 0.0430 - val_cosine_similarity: 0.9019 - val_loss: 0.0426\n",
      "Epoch 4/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 34ms/step - cosine_similarity: 0.9012 - loss: 0.0417 - val_cosine_similarity: 0.9018 - val_loss: 0.0405\n",
      "Epoch 5/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - cosine_similarity: 0.8991 - loss: 0.0410 - val_cosine_similarity: 0.8914 - val_loss: 0.0471\n",
      "Epoch 6/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - cosine_similarity: 0.9008 - loss: 0.0407 - val_cosine_similarity: 0.9019 - val_loss: 0.0412\n",
      "Epoch 7/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - cosine_similarity: 0.9001 - loss: 0.0398 - val_cosine_similarity: 0.8953 - val_loss: 0.0411\n",
      "Epoch 8/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - cosine_similarity: 0.9023 - loss: 0.0394 - val_cosine_similarity: 0.9012 - val_loss: 0.0386\n",
      "Epoch 9/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 31ms/step - cosine_similarity: 0.9018 - loss: 0.0389 - val_cosine_similarity: 0.9032 - val_loss: 0.0379\n",
      "Epoch 10/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - cosine_similarity: 0.9007 - loss: 0.0385 - val_cosine_similarity: 0.9022 - val_loss: 0.0370\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 300\n",
    "hidden_dim = 128\n",
    "max_sentence_len = 20  # Max sequence length for padding\n",
    "\n",
    "# Preprocess: Tokenize and embed sentences\n",
    "def embed_sentence(sentence):\n",
    "    tokens = word_tokenize(sentence.lower())\n",
    "    # embeddings = [word2vec[word] for word in tokens if word in word2vec]\n",
    "    embeddings = [word2vec.wv[word] for word in tokens if word in word2vec.wv]\n",
    "    return embeddings\n",
    "\n",
    "# Convert sentences to embeddings\n",
    "embedded_sentences = [embed_sentence(sent) for sent in sentences]\n",
    "# Pad the sentences to have uniform length\n",
    "embedded_sentences = pad_sequences(\n",
    "    [np.array(e) for e in embedded_sentences if len(e) > 0], \n",
    "    maxlen=max_sentence_len, \n",
    "    dtype='float32', \n",
    "    padding='post'\n",
    ")\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test = train_test_split(embedded_sentences, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the Autoencoder Model\n",
    "input_dim = (max_sentence_len, embedding_dim)\n",
    "\n",
    "input_layer = Input(shape=(max_sentence_len, embedding_dim))\n",
    "encoded = LSTM(128, return_sequences=False, kernel_regularizer=l2(0.01))(input_layer)\n",
    "encoded = Dropout(0.3)(encoded)\n",
    "encoded = BatchNormalization()(encoded)\n",
    "encoded = RepeatVector(max_sentence_len)(encoded)\n",
    "decoded = LSTM(128, return_sequences=True, kernel_regularizer=l2(0.01))(encoded)\n",
    "decoded = Dropout(0.3)(decoded)\n",
    "output_layer = TimeDistributed(Dense(embedding_dim))(decoded)\n",
    "\n",
    "autoencoder = Model(input_layer, output_layer)\n",
    "autoencoder.compile(optimizer='adam', loss='mse', metrics=[CosineSimilarity(axis=-1)])\n",
    "\n",
    "# Define the model\n",
    "# input_layer = tf.keras.Input(shape=input_dim)\n",
    "\n",
    "# encoded = Bidirectional(LSTM(hidden_dim, return_sequences=False))(input_layer)\n",
    "# encoded = LSTM(hidden_dim, return_sequences=False)(input_layer)\n",
    "# encoded = RepeatVector(max_sentence_len)(encoded)\n",
    "# decoded = LSTM(hidden_dim, return_sequences=True)(encoded)\n",
    "# output_layer = TimeDistributed(Dense(embedding_dim))(decoded)\n",
    "\n",
    "# autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "# autoencoder.compile(optimizer='adam', loss='mse',metrics=[CosineSimilarity(axis=-1)])\n",
    "# Train the Autoencoder\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "history = autoencoder.fit(X_train, X_train,\n",
    "                          epochs=num_epochs,\n",
    "                          batch_size=batch_size,\n",
    "                          validation_data=(X_test, X_test))\n",
    "\n",
    "# Evaluate\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a07679d-98ac-42e5-9bff-b7d6f752eaaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['cosine_similarity', 'loss', 'val_cosine_similarity', 'val_loss'])\n",
      "Final Training Loss: 0.038399066776037216\n",
      "Final Validation Loss: 0.03698806092143059\n",
      "Final Training Accuracy (Cosine Similarity): 0.9025304913520813\n",
      "Final Validation Accuracy (Cosine Similarity): 0.9021615386009216\n"
     ]
    }
   ],
   "source": [
    "training_loss = history.history['loss']\n",
    "validation_loss = history.history['val_loss']\n",
    "training_accuracy = history.history['cosine_similarity']\n",
    "validation_accuracy = history.history['val_cosine_similarity']\n",
    "\n",
    "# Print final results\n",
    "print(history.history.keys())\n",
    "print(f\"Final Training Loss: {training_loss[-1]}\")\n",
    "print(f\"Final Validation Loss: {validation_loss[-1]}\")\n",
    "print(f\"Final Training Accuracy (Cosine Similarity): {training_accuracy[-1]}\")\n",
    "print(f\"Final Validation Accuracy (Cosine Similarity): {validation_accuracy[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5eb5983-12bb-4ce4-b442-a40dc11703d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
